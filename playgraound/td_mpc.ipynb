{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Hyperparameters\n",
    "H = 5  # Planning horizon\n",
    "N = 512  # Number of sampled trajectories\n",
    "K = 64  # Top-k trajectories\n",
    "J = 6  # Planning iterations\n",
    "GAMMA = 0.99  # Discount factor\n",
    "LAMBDA = 0.5  # Temporal weighting\n",
    "C1, C2, C3 = 0.5, 0.1, 2.0  # Loss coefficients\n",
    "BATCH_SIZE = 512\n",
    "BUFFER_SIZE = 10000\n",
    "SEED_STEPS = 1000\n",
    "\n",
    "# Neural Network for Task-Oriented Latent Dynamics (TOLD) components\n",
    "class TOLD(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, latent_dim=50):\n",
    "        super(TOLD, self).__init__()\n",
    "        self.state_encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256), nn.ReLU(), nn.Linear(256, latent_dim)\n",
    "        )  # Encodes state to latent space\n",
    "        self.dynamics_predictor = nn.Sequential(\n",
    "            nn.Linear(latent_dim + action_dim, 512), nn.ReLU(), nn.Linear(512, latent_dim)\n",
    "        )  # Predicts next latent state\n",
    "        self.reward_predictor = nn.Sequential(\n",
    "            nn.Linear(latent_dim + action_dim, 512), nn.ReLU(), nn.Linear(512, 1)\n",
    "        )  # Predicts reward\n",
    "        self.value_estimator = nn.Sequential(\n",
    "            nn.Linear(latent_dim + action_dim, 512), nn.ReLU(), nn.Linear(512, 1)\n",
    "        )  # Estimates Q-value\n",
    "        self.policy_network = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512), nn.ReLU(), nn.Linear(512, action_dim)\n",
    "        )  # Suggests actions\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        z = self.state_encoder(s)\n",
    "        za = torch.cat([z, a], dim=-1)\n",
    "        z_next = self.dynamics_predictor(za)\n",
    "        r_hat = self.reward_predictor(za)\n",
    "        q_hat = self.value_estimator(za)\n",
    "        a_hat = self.policy_network(z)\n",
    "        return z, z_next, r_hat, q_hat, a_hat\n",
    "\n",
    "# Environment setup\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = 1  # Simplified to continuous action in [-1, 1]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model and optimizer\n",
    "model = TOLD(state_dim, action_dim).to(device)\n",
    "target_model = TOLD(state_dim, action_dim).to(device)\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Replay buffer\n",
    "buffer = deque(maxlen=BUFFER_SIZE)\n",
    "\n",
    "# MPPI Planning\n",
    "def plan(model, s, horizon=H, n_samples=N, n_elite=K, n_iter=J):\n",
    "    s_tensor = torch.FloatTensor(s).unsqueeze(0).to(device)\n",
    "    z = model.state_encoder(s_tensor)\n",
    "    mu, sigma = torch.zeros(horizon, action_dim).to(device), 2 * torch.ones(horizon, action_dim).to(device)\n",
    "    \n",
    "    for _ in range(n_iter):\n",
    "        actions = torch.randn(n_samples, horizon, action_dim).to(device) * sigma + mu\n",
    "        returns = torch.zeros(n_samples).to(device)\n",
    "        \n",
    "        z_t = z.repeat(n_samples, 1)\n",
    "        for t in range(horizon):\n",
    "            a_t = actions[:, t]\n",
    "            za_t = torch.cat([z_t, a_t], dim=-1)\n",
    "            r_t = model.reward_predictor(za_t).squeeze(-1)\n",
    "            z_next = model.dynamics_predictor(za_t)\n",
    "            returns += (GAMMA ** t) * r_t\n",
    "            z_t = z_next\n",
    "        \n",
    "        # Add terminal value\n",
    "        a_H = model.policy_network(z_t)\n",
    "        q_H = model.value_estimator(torch.cat([z_t, a_H], dim=-1)).squeeze(-1)\n",
    "        returns += (GAMMA ** horizon) * q_H\n",
    "        \n",
    "        # Update distribution\n",
    "        topk_idx = torch.topk(returns, n_elite, dim=0)[1]\n",
    "        topk_actions = actions[topk_idx]\n",
    "        weights = torch.exp(0.5 * returns[topk_idx])  # Temperature = 0.5\n",
    "        weights /= weights.sum()\n",
    "        mu = (weights.unsqueeze(-1) * topk_actions).sum(dim=0)\n",
    "        sigma = torch.sqrt(((topk_actions - mu)**2 * weights.unsqueeze(-1)).sum(dim=0))\n",
    "        sigma = torch.clamp(sigma, min=0.05)  # Exploration constraint\n",
    "    \n",
    "    return (mu[0] + sigma[0] * torch.randn(action_dim).to(device)).cpu().numpy()\n",
    "\n",
    "# Training loop\n",
    "total_steps = 0\n",
    "for episode in range(500):\n",
    "    s = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        total_steps += 1\n",
    "        if total_steps < SEED_STEPS:\n",
    "            a = env.action_space.sample()  # Random action\n",
    "        else:\n",
    "            a = plan(model, s)  # Plan with TD-MPC\n",
    "        \n",
    "        a = np.clip(a, -1, 1)  # Simplified action range\n",
    "        s_next, r, done, _ = env.step(a[0] * 2)  # Scale to CartPole's discrete action\n",
    "        buffer.append((s, a, r, s_next))\n",
    "        episode_reward += r\n",
    "        s = s_next\n",
    "        \n",
    "        if len(buffer) >= BATCH_SIZE and total_steps >= SEED_STEPS:\n",
    "            batch = random.sample(buffer, BATCH_SIZE)\n",
    "            s_batch, a_batch, r_batch, s_next_batch = map(lambda x: torch.FloatTensor(x).to(device), zip(*batch))\n",
    "            \n",
    "            # Forward pass\n",
    "            z, z_next, r_hat, q_hat, a_hat = model(s_batch, a_batch)\n",
    "            with torch.no_grad():\n",
    "                _, z_next_target, _, q_next_target, _ = target_model(s_next_batch, model.policy_network(z_next))\n",
    "            \n",
    "            # Losses\n",
    "            reward_loss = C1 * (r_hat - r_batch).pow(2).mean()\n",
    "            value_loss = C2 * (q_hat - (r_batch + GAMMA * q_next_target)).pow(2).mean()\n",
    "            consistency_loss = C3 * (z_next - target_model.state_encoder(s_next_batch)).pow(2).mean()\n",
    "            policy_loss = -model.value_estimator(torch.cat([z.detach(), model.policy_network(z)], dim=-1)).mean()\n",
    "            total_loss = reward_loss + value_loss + consistency_loss + policy_loss\n",
    "            \n",
    "            # Update\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update target network\n",
    "            for target_param, param in zip(target_model.parameters(), model.parameters()):\n",
    "                target_param.data.copy_(0.99 * target_param.data + 0.01 * param.data)\n",
    "    \n",
    "    print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "H = 5  # Planning horizon\n",
    "N = 512  # Number of sampled trajectories\n",
    "K = 64  # Top-k trajectories\n",
    "J = 6  # Planning iterations\n",
    "GAMMA = 0.99  # Discount factor\n",
    "LAMBDA = 0.5  # Temporal weighting\n",
    "C1, C2, C3 = 0.5, 0.1, 2.0  # Loss coefficients\n",
    "BATCH_SIZE = 512\n",
    "BUFFER_SIZE = 10000\n",
    "SEED_STEPS = 1000\n",
    "\n",
    "# Neural Network for Task-Oriented Latent Dynamics (TOLD) components\n",
    "class TOLD(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, latent_dim=50):\n",
    "        super(TOLD, self).__init__()\n",
    "        self.state_encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256), nn.ReLU(), nn.Linear(256, latent_dim)\n",
    "        )  # Encodes state to latent space\n",
    "        self.dynamics_predictor = nn.Sequential(\n",
    "            nn.Linear(latent_dim + action_dim, 512), nn.ReLU(), nn.Linear(512, latent_dim)\n",
    "        )  # Predicts next latent state\n",
    "        self.reward_predictor = nn.Sequential(\n",
    "            nn.Linear(latent_dim + action_dim, 512), nn.ReLU(), nn.Linear(512, 1)\n",
    "        )  # Predicts reward\n",
    "        self.value_estimator = nn.Sequential(\n",
    "            nn.Linear(latent_dim + action_dim, 512), nn.ReLU(), nn.Linear(512, 1)\n",
    "        )  # Estimates Q-value\n",
    "        self.policy_network = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 512), nn.ReLU(), nn.Linear(512, action_dim)\n",
    "        )  # Suggests actions\n",
    "\n",
    "    def forward(self, s, a):\n",
    "        z = self.state_encoder(s)\n",
    "        za = torch.cat([z, a], dim=-1)\n",
    "        z_next = self.dynamics_predictor(za)\n",
    "        r_hat = self.reward_predictor(za)\n",
    "        q_hat = self.value_estimator(za)\n",
    "        a_hat = self.policy_network(z)\n",
    "        return z, z_next, r_hat, q_hat, a_hat\n",
    "\n",
    "# Environment setup\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = 1  # Simplified to continuous action in [-1, 1]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model and optimizer\n",
    "model = TOLD(state_dim, action_dim).to(device)\n",
    "target_model = TOLD(state_dim, action_dim).to(device)\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Replay buffer\n",
    "buffer = deque(maxlen=BUFFER_SIZE)\n",
    "\n",
    "# MPPI Planning\n",
    "def plan(model, s, horizon=H, n_samples=N, n_elite=K, n_iter=J):\n",
    "    s_tensor = torch.FloatTensor(s).unsqueeze(0).to(device)\n",
    "    z = model.state_encoder(s_tensor)\n",
    "    mu, sigma = torch.zeros(horizon, action_dim).to(device), 2 * torch.ones(horizon, action_dim).to(device)\n",
    "    \n",
    "    for _ in range(n_iter):\n",
    "        actions = torch.randn(n_samples, horizon, action_dim).to(device) * sigma + mu\n",
    "        returns = torch.zeros(n_samples).to(device)\n",
    "        \n",
    "        z_t = z.repeat(n_samples, 1)\n",
    "        for t in range(horizon):\n",
    "            a_t = actions[:, t]\n",
    "            za_t = torch.cat([z_t, a_t], dim=-1)\n",
    "            r_t = model.reward_predictor(za_t).squeeze(-1)\n",
    "            z_next = model.dynamics_predictor(za_t)\n",
    "            returns += (GAMMA ** t) * r_t\n",
    "            z_t = z_next\n",
    "        \n",
    "        # Add terminal value\n",
    "        a_H = model.policy_network(z_t)\n",
    "        q_H = model.value_estimator(torch.cat([z_t, a_H], dim=-1)).squeeze(-1)\n",
    "        returns += (GAMMA ** horizon) * q_H\n",
    "        \n",
    "        # Update distribution\n",
    "        topk_idx = torch.topk(returns, n_elite, dim=0)[1]\n",
    "        topk_actions = actions[topk_idx]\n",
    "        weights = torch.exp(0.5 * returns[topk_idx])  # Temperature = 0.5\n",
    "        weights /= weights.sum()\n",
    "        mu = (weights.unsqueeze(-1) * topk_actions).sum(dim=0)\n",
    "        sigma = torch.sqrt(((topk_actions - mu)**2 * weights.unsqueeze(-1)).sum(dim=0))\n",
    "        sigma = torch.clamp(sigma, min=0.05)  # Exploration constraint\n",
    "    \n",
    "    return (mu[0] + sigma[0] * torch.randn(action_dim).to(device)).cpu().numpy()\n",
    "\n",
    "# Training loop\n",
    "total_steps = 0\n",
    "for episode in range(500):\n",
    "    s = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        total_steps += 1\n",
    "        if total_steps < SEED_STEPS:\n",
    "            a = env.action_space.sample()  # Random action\n",
    "        else:\n",
    "            a = plan(model, s)  # Plan with TD-MPC\n",
    "        \n",
    "        a = np.clip(a, -1, 1)  # Simplified action range\n",
    "        s_next, r, done, _ = env.step(a[0] * 2)  # Scale to CartPole's discrete action\n",
    "        buffer.append((s, a, r, s_next))\n",
    "        episode_reward += r\n",
    "        s = s_next\n",
    "        \n",
    "        if len(buffer) >= BATCH_SIZE and total_steps >= SEED_STEPS:\n",
    "            batch = random.sample(buffer, BATCH_SIZE)\n",
    "            s_batch, a_batch, r_batch, s_next_batch = map(lambda x: torch.FloatTensor(x).to(device), zip(*batch))\n",
    "            \n",
    "            # Forward pass\n",
    "            z, z_next, r_hat, q_hat, a_hat = model(s_batch, a_batch)\n",
    "            with torch.no_grad():\n",
    "                _, z_next_target, _, q_next_target, _ = target_model(s_next_batch, model.policy_network(z_next))\n",
    "            \n",
    "            # Losses\n",
    "            reward_loss = C1 * (r_hat - r_batch).pow(2).mean()\n",
    "            value_loss = C2 * (q_hat - (r_batch + GAMMA * q_next_target)).pow(2).mean()\n",
    "            consistency_loss = C3 * (z_next - target_model.state_encoder(s_next_batch)).pow(2).mean()\n",
    "            policy_loss = -model.value_estimator(torch.cat([z.detach(), model.policy_network(z)], dim=-1)).mean()\n",
    "            total_loss = reward_loss + value_loss + consistency_loss + policy_loss\n",
    "            \n",
    "            # Update\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update target network\n",
    "            for target_param, param in zip(target_model.parameters(), model.parameters()):\n",
    "                target_param.data.copy_(0.99 * target_param.data + 0.01 * param.data)\n",
    "    \n",
    "    print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
